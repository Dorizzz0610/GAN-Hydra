
### What is the role of the discriminator in a GAN model? Use this project's discriminator as an example.
In a GAN model, the discriminator is to distinguish between real images from the dataset and fake images generated by the generator. It is trained to best enhance its ability to correctly classify the images as either real or fake. 
In our GAN project, the discriminator receives either real images from MNIST or fake images produced by the generator. It evaluates these images and outputs a single value, which represents its confidence that the given image is real. 
Our discriminator is under conditional GAN, where the model uses label information to generate and discriminate images based on specific conditions, since these label embeddings can provide detailed class-specific information. In our case, GAN is provided the additional information of digit lable. The concatenated input undergoes several transformations through convolutional layers, LeakyReLU activations, and dropout layers, before being pooled and flattened into a final output that predicts the "realness" of the input image.



### The generator network in this code base takes two arguments: `noise` and `labels`. What are these inputs and how could they be used at inference time to generate an image of the number 5?
Noise: This is a random vector that provides a source of randomness in the images generated by the GAN. It is essential for ensuring that the generated images are diverse. With noise, generators can better learn the whole deep pattern and data distribution, instead of replicating images.
Labels: These are used in conditional GANs where the generator needs to generate images that correspond to specific conditions. In this project, labels are the digits 0-9.
At inference time, the generator then uses these inputs to produce an image that it believes to correspond to the label '5', as learned during the training phase. The noise vector allows for the generation of various versions of '5'.

### What steps are needed to deploy a model into production?
1. Begin by researching and selecting the most suitable model architecture for the specific task. 
2. Gather, clean, and preprocess the data. This includes feature engineering and reducing dimensionality etc.
3. Train the model and use techniques like cross-validation to tune hyperparameters and avoid overfitting.
4. Once the model training is finalized, convert the model into a format suitable for deployment, like ONNX (Open Neural Network Exchange). Also, freeze the model weights to prevent further changes.
5. Optimize the model for the production environment, like reducing the model’s size and computational costs.
6. Decide on the deployment strategy: whether it’s in a private or public cloud environment, or via edge devices.
7. Develop an API or docker image to enable easy access and integration of the model into existing systems. 
8. Conduct testing to ensure the model integrates well with the production systems and performs as expected under various real-world conditions.


### If you wanted to train with multiple GPUs, what can you do in pytorch lightning to make sure data is allocated to the correct GPU? 
Trainer class has the `gpus` parameter, which specifies the number of GPUs to use. For example, Trainer(gpus=4) will use four GPUs.
Also the `accelerator` parameter can be set to to "dp" for Data Parallelism, where the model is replicated on each GPU, and each GPU processes a portion of the input data.


### The output `wandb` graphs and images resulting from your training run. You are also invited to talk about difficulties you encountered and how you overcame them
1. Vanilla GAN
    https://wandb.ai/doriszhang/Tests/reports/gen_imgs-24-05-20-12-53-28---Vmlldzo4MDE3OTQ0?accessToken=jhpb2g1v7tak9k5xzk7l9lqinfdgvpqfies6wqov0vataw8cpopbxzcv8yvscpdp
    ![image](/images/Vanilla%20GAN.png)
2. GAN with ResBlock
    https://wandb.ai/doriszhang/Tests/reports/gen_imgs-24-05-20-12-45-42---Vmlldzo4MDE3ODk0?accessToken=wwk3jdeknt6e63yk01pp1ndmyfzqowhpuf6wn9wf17asjd4j67zm11u7osvrzw8k
    ![image](/images/GAN%20with%20ResBlock.png)

Issues:
1. I encountered issues starting from package installation, like there are incompatible C compiler on my MacOS and there are aliased Python overriding conda Python, etc.  I researched and applied solutions such as updating system paths and configuring environmental variables, ensuring compatibility with my development tools. Additionally, I resolved conflicts caused by an aliased Python version by reordering my PATH environment variable to prioritize Conda's Python.
2. I wasn't very familiar with GAN initially. I seek out educational resources, including tutorial videos and sample code. Through these studies, I not only learned the fundamentals of GAN but also explored its variants.
3. I tried to improve Vanilla GAN's performance. Previously I implemented ResNet in a task so I tried to integrate ResBlock into GAN this time. However, I met the issue that the enhanced version generates worse images. After my reflection, there are some possible explanations: 
* MNIST is a relatively simple dataset. Incorporating ResBlocks may add too much complexity.
* ResBlocks increase the capacity of the network, which may lead to overfitting.
* GANs is very unstable, and adding ResBlocks might exacerbate these issues by introducing more non-linearities and dependencies between layers.
I decided to remove the ResBlocks then.




# Chatbot Assignment:

### Compare atleast 3 different models and provide insights on Content Quality, Contextual Understanding, Language Fluency and Ethical Considerations with examples.
I compare for LLaMa-3, Gemma and Zephyr. Detailed prompts, answers, and comparison analysis are written in this notion page due to space limit. https://proud-nutria-51b.notion.site/HuggingChat-models-comparison-97e03dedbe7c401c8571c8fc9f59fb07?pvs=4


### What are the parameters that can be used to control response. Explain in detail.
1. Temperature: Influences the randomness or "surprise" in the response. A lower temperature produces more predictable responses, while a higher temperature generates more diverse and potentially creative outputs.
2. Max Tokens: Determines the maximum length of the response. Setting a higher max tokens allows for longer responses, useful for detailed explanations.
3. Top-k Sampling: Limits the model to only consider the top k most likely next words at each step of generation. This reduces the chance of some ridiculously unlikely word choices.
4. Top-p Sampling: This parameter uses a cumulative probability threshold. The model considers a variable number of words until their combined probability exceeds p. This approach balances diversity and relevance.
5. Stop Sequences: These are specific tokens or phrases at which the model will stop generating further text, like “.” or special markers.

### Explore various techniques used in prompt engineering, such as template-based prompts, rule-based prompts, and machine learning-based prompts and provide what are the challenges and considerations in designing effective prompts with examples.
1. Template-Based Prompts: Use fixed templates, where we can insert some variable elements. This method is consistent and can be highly effective for structured tasks. However, it has issue of inflexibility, and the model may fall into fixed mindset.
2. Rule-Based Prompts: Define specific rules for how prompts should be constructed based on the input or context. This is more flexible comparing to template-based prompts. However, rules fail to cover all the potential cases.
3. Machine Learning-Based Prompts: Use machine learning techniques to dynamically generate prompts. This can better fit diverse tasks. However, we need to make sure the training data for this machine learning model is diverse and general, without bias, otherwise it will have the same drawbacks as the two techniques above.
4. Challenges and considerations in designing effective prompts
* Too complex or too long prompts may lead to confusion or irrelevant responses. Instead of a complex prompt like "Given the current climate state and current trade agreement conditions, what might be the implications of climate change for international trade agreements?", simplify it to "How could climate change impact international trade agreements?"
* Prompts need to be clear and specific to avoid ambiguousness, so as to allow the model to capture the essence of the task. If a prompt is too vague, such as "Tell me about London," the model might not understand whether to focus on history or geography. A more specific prompt would be, "What are the major historical landmarks in London?"
*  Prompts must minimize the introduction of biases. For example, "What do you think of the annoying animal pigs?" create more bias than  "What do you think of pigs?"
* While open-ended prompts can stimulate creativity or deeper analysis, they might also lead to too broad or unfocused responses. An open-ended example is "Discuss the effects of the Internet" could while more focused one would be "Analyze the impact of the Internet on educational resources accessibility." This can help the model better capture what it's supposed to do.

### What is retrieval-augmented generation(RAG) and how is it applied in natural language generation tasks?
RAG is a method of enhancing generation by integrating a retrieval step. Before generating a response, the system retrieves information from relevant documents or corpus. The retrieved information is then used as augmentation for the model, better shaping the response. In tasks like article writing, RAG can retrieve background information or data, ensuring the content is rich and appropriate.
Typically, RAG uses a neural network to encode both the query and the documents into a shared space. The most relevant documents are selected based on similarity to the query in this space, and then be used during the generation process.



